{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEhELqcqSTv-",
        "outputId": "2f73b224-5b74-4d59-9d2a-426839c5b825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import openai\n",
        "import re\n",
        "import base64\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"x\"\n",
        "\n",
        "\n",
        "FOLDER_PATH = \"/content/drive/MyDrive/reasoning_multimodal_LLMs/example_data\"\n",
        "IMG_PATH = \"/content/drive/MyDrive/MATH-V-main\"\n",
        "\n",
        "def load_qwen_model():\n",
        "    !pip install git+https://github.com/huggingface/transformers accelerate\n",
        "    !pip install transformers[qwen] --upgrade\n",
        "    !pip install qwen-vl-utils[decord]==0.0.8\n",
        "    !pip install flash-attn\n",
        "    from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "    from qwen_vl_utils import process_vision_info\n",
        "\n",
        "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "        \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
        "        torch_dtype=torch.bfloat16, # 'auto', #\n",
        "        attn_implementation=\"flash_attention_2\",\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    # The default range for the number of visual tokens per image in the model is 4-16384.\n",
        "    # You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
        "    # min_pixels = 256*28*28\n",
        "    # max_pixels = 1280*28*28\n",
        "    processor = AutoProcessor.from_pretrained(\n",
        "        \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
        "        # min_pixels=min_pixels,\n",
        "        # max_pixels=256*28*28\n",
        "    )\n",
        "    return model, processor\n",
        "\n",
        "# model, processor = load_qwen_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI9J7L9GuWBs",
        "outputId": "92533e1a-013a-466e-df12-f3920581fd55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|█████▉    | 1640/2736 [9:10:09<9:03:41, 29.76s/it]"
          ]
        }
      ],
      "source": [
        "def split_convert_mathvision_to_json(split):\n",
        "    # Load the dataset\n",
        "    ds = load_dataset(\"MathLLMs/MathVision\")\n",
        "\n",
        "    # Combine train and test splits for reshuffling\n",
        "    train_data = []\n",
        "    test_data = []\n",
        "\n",
        "    test_mini_ids = ds['testmini']['id']\n",
        "    for item in ds[split]:\n",
        "        if item['id'] in test_mini_ids:\n",
        "            test_data.append(item)\n",
        "        else:\n",
        "            train_data.append(item)\n",
        "\n",
        "    # Shuffle and split the data (80% train, 20% test)\n",
        "    # train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Process each split\n",
        "    train_json = process_split(train_data, 'train', generate_solution=True)\n",
        "    test_json = process_split(test_data, 'test', generate_solution=False)\n",
        "\n",
        "    # Save to files\n",
        "    with open(f\"{FOLDER_PATH}/mathvision_train.json\", 'w') as f:\n",
        "        json.dump(train_json, f, indent=2)\n",
        "\n",
        "    with open(f\"{FOLDER_PATH}/mathvision_test.json\", 'w') as f:\n",
        "        json.dump(test_json, f, indent=2)\n",
        "\n",
        "    print(f\"Converted {len(train_json)} entries for train split\")\n",
        "    print(f\"Converted {len(test_json)} entries for test split\")\n",
        "\n",
        "    return train_json, test_json\n",
        "\n",
        "def convert_mathvision_to_json(split):\n",
        "    # Load the dataset\n",
        "    ds = load_dataset(\"MathLLMs/MathVision\")\n",
        "    data_json = process_split(ds[split], split)\n",
        "\n",
        "    # Save to files\n",
        "    with open(f\"{FOLDER_PATH}/mathvision_{split}.json\", 'w') as f:\n",
        "        json.dump(data_json, f, indent=2)\n",
        "\n",
        "    print(f\"Converted {len(data_json)} entries for test split\")\n",
        "\n",
        "    return data_json\n",
        "\n",
        "def find_solution(model_response, item):\n",
        "    # Extract the answer from the response (assuming it's in quotes)\n",
        "    match = re.search(r\"'([^']+)'(\\.|\\s)*$\", model_response)\n",
        "    model_answer = match.group(1) if match else None\n",
        "\n",
        "    # Check if the answer is correct\n",
        "    correct_answer = item.get('answer', '').strip()\n",
        "    is_correct = model_answer and model_answer.strip().lower() == correct_answer.lower()\n",
        "\n",
        "    return model_response if is_correct else f\"\"\"The correct answer is: '{correct_answer}' \"\"\"\n",
        "\n",
        "def process_split(data, split, generate_solution = False):\n",
        "    converted_data = []\n",
        "    instruction = \"Answer the following question using a single word or phrase, by considering the image provided.\"\n",
        "    # instruction =\n",
        "    for i, item in enumerate(tqdm(data)):\n",
        "        question_prompt = f\"\"\"Please solve the problem step by step and put your final answer and the end of the solution in single quotes. If it is a multiple choice question, only one letter is allowed in the quotes. \\n {item['question']}\"\"\"\n",
        "        if item.get('options') and len(item['options']) > 0:\n",
        "            question_prompt += f\". Choose from the options {', '.join(item['options'][:-1])}, or {item['options'][-1]}.\"\n",
        "\n",
        "        image_path = f\"{IMG_PATH}/{item.get('image')}\"\n",
        "        if generate_solution:\n",
        "            # First, get the model's answer\n",
        "            # model_response = query_gpt4v(image_path, question_prompt)\n",
        "            model_response = query_qwen(image_path, question_prompt, \"\")\n",
        "\n",
        "            final_solution = find_solution(model_response, item)\n",
        "\n",
        "            # print(f\"model_response = {model_response}\")\n",
        "            # print(f\"final_solution = {final_solution}\")\n",
        "        else:\n",
        "            final_solution = f\"\"\"The correct answer is: '{item.get('answer', '').strip()}' \"\"\"\n",
        "        # For fine-tuning, include full solution if correct\n",
        "        conversation_entry = {\n",
        "            \"system_prompt\": \"You are a helpful visual assistant that can understand images and answer questions about them accurately and concisely. \" + instruction,\n",
        "            \"image\": item.get(\"image\"),\n",
        "            \"conversations\": [\n",
        "                {\n",
        "                    \"from\": \"human\",\n",
        "                    \"value\": f\"<image>\\n{question_prompt}\"\n",
        "                },\n",
        "                {\n",
        "                    \"from\": \"gpt\",\n",
        "                    \"value\": final_solution\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        converted_data.append(conversation_entry)\n",
        "        if i%50 == 0:\n",
        "            with open(f\"{FOLDER_PATH}/mathvision_{split}.json\", 'w') as f:\n",
        "                json.dump(converted_data, f, indent=2)\n",
        "\n",
        "    return converted_data\n",
        "\n",
        "\n",
        "def query_qwen(image_path, prompt, instruction):\n",
        "    messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful visual assistant that can understand images and answer questions about them accurately and concisely.\" + instruction\n",
        "            },\n",
        "             {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image_path},\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Preparation for inference\n",
        "    text = processor.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    # Inference: Generation of the output\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    output_text = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    return output_text[0]\n",
        "\n",
        "\n",
        "\n",
        "# Helper function to query GPT4-V (you'll need to implement this based on your API access)\n",
        "def query_gpt4v(image_path, prompt):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "          base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4-vision-preview\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful visual assistant that can understand images and answer questions about them accurately and concisely.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# Run the conversion\n",
        "train_json, test_json = split_convert_mathvision_to_json('test')\n",
        "\n",
        "# # Convert testmini split\n",
        "# testmini_json = convert_mathvision_to_json('testmini')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBxlTQPPtKFx"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"MathLLMs/MathVision\")['testmini']\n",
        "ds\n",
        "counter = 0\n",
        "for item in tqdm(ds):\n",
        "    if '<image2>' in item['question']:\n",
        "        print(item['id'])\n",
        "        print(item['question'])\n",
        "        print(item['image'])\n",
        "        print('----------')\n",
        "        counter += 1\n",
        "print(counter)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{FOLDER_PATH}/mathvision_train.json\", 'r') as f:\n",
        "    train_json = json.load(f)\n",
        "json_item = train_json[0]\n",
        "json_item['conversations'][0]['value']"
      ],
      "metadata": {
        "id": "ffapnckV7rq4",
        "outputId": "0cefae3c-99ba-440a-f727-49ed67a7e5fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<image>\\nPlease solve the problem step by step and put your final answer and the end of the solution in single quotes. If it is a multiple choice question, only one letter is allowed in the quotes. \\n Which number should be written in place of the question mark?\\n<image1>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IlNI2yWaSLpL",
        "outputId": "d45f7c76-c2b2-46c2-f033-692e560bd472",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3040/3040 [00:07<00:00, 433.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 180 questions in the train split with solutions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "ds = load_dataset(\"MathLLMs/MathVision\")\n",
        "\n",
        "with open(f\"{FOLDER_PATH}/mathvision_train.json\", 'r') as f:\n",
        "    train_json = json.load(f)\n",
        "\n",
        "# Combine train and test splits for reshuffling\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "test_mini_ids = ds['testmini']['id']\n",
        "train_counter = 0\n",
        "with_solution_counter = 0\n",
        "for item in tqdm(ds['test']):\n",
        "    if item['id'] in test_mini_ids:\n",
        "        test_data.append(item)\n",
        "    else:\n",
        "        if train_counter < len(train_json):\n",
        "          json_item = train_json[train_counter]\n",
        "          if (\n",
        "              item['question'] in json_item.get('conversations')[0]['value']\n",
        "              and not json_item['conversations'][1]['value'].startswith('The correct answer is:')\n",
        "          ):\n",
        "              item['solution'] = json_item['conversations'][1]['value']\n",
        "              with_solution_counter += 1\n",
        "        train_data.append(item)\n",
        "        train_counter += 1\n",
        "\n",
        "\n",
        "print(f'There are {with_solution_counter} questions in the train split with solutions')\n",
        "pd.DataFrame(train_data).to_csv(f\"{FOLDER_PATH}/mathvision_train.csv\", index=False)\n",
        "pd.DataFrame(test_data).to_csv(f\"{FOLDER_PATH}/mathvision_test.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1 (main, Dec  3 2024, 17:59:52) [Clang 16.0.0 (clang-1600.0.26.4)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a705a04fb367ec22af69d2ff9887b181893499ca251cf5bc866022978b76dc8e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}