{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f653d92fcd94433ab7a8fc598426d87f",
            "bb1984d325714c0e99d67a31bdfeead4",
            "f95e1b23a15549b480f39b63b8744709",
            "9031cc6f67df49f3912febfa658f215d",
            "ef8f08019c5f4ac3916013d99eaecb0b",
            "adc6e52810bf49238a7905292ded866b",
            "18ee6ea0c62b42a48b8d108aff4d578a",
            "df74531141d146eca0ecd399ba89b0c4",
            "e54eb2bfb090448ba77361b7315b6f71",
            "7bc7f36b4213469ca12dc449880169dd",
            "cddaea09cf624068b9a104a4ef65dbb5"
          ]
        },
        "id": "bEhELqcqSTv-",
        "outputId": "a2d927bb-b526-45a3-dec9-365cd17fb1ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-dvduxs6v\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-dvduxs6v\n",
            "  Resolved https://github.com/huggingface/transformers to commit 92c5ca9dd70de3ade2af2eb835c96215cc50e815\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: transformers[qwen] in /usr/local/lib/python3.11/dist-packages (4.50.0.dev0)\n",
            "\u001b[33mWARNING: transformers 4.50.0.dev0 does not provide the extra 'qwen'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[qwen]) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[qwen]) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[qwen]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[qwen]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[qwen]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[qwen]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[qwen]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[qwen]) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers[qwen]) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[qwen]) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers[qwen]) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers[qwen]) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[qwen]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[qwen]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[qwen]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[qwen]) (2025.1.31)\n",
            "Requirement already satisfied: qwen-vl-utils==0.0.8 in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils[decord]==0.0.8) (0.0.8)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (14.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (24.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (11.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2.32.3)\n",
            "Requirement already satisfied: decord in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils[decord]==0.0.8) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from decord->qwen-vl-utils[decord]==0.0.8) (1.26.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils==0.0.8->qwen-vl-utils[decord]==0.0.8) (2025.1.31)\n",
            "Requirement already satisfied: flash-attn in /usr/local/lib/python3.11/dist-packages (2.7.4.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.5.1+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f653d92fcd94433ab7a8fc598426d87f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import openai\n",
        "import re\n",
        "import base64\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"x\"\n",
        "\n",
        "\n",
        "FOLDER_PATH = \"/content/drive/MyDrive/reasoning_multimodal_LLMs/example_data\"\n",
        "IMG_PATH = \"/content/drive/MyDrive/MATH-V-main\"\n",
        "\n",
        "def load_qwen_model():\n",
        "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "        \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
        "        torch_dtype=torch.bfloat16, # 'auto', #\n",
        "        attn_implementation=\"flash_attention_2\",\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    # The default range for the number of visual tokens per image in the model is 4-16384.\n",
        "    # You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
        "    # min_pixels = 256*28*28\n",
        "    # max_pixels = 1280*28*28\n",
        "    processor = AutoProcessor.from_pretrained(\n",
        "        \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
        "        # min_pixels=min_pixels,\n",
        "        # max_pixels=256*28*28\n",
        "    )\n",
        "    return model, processor\n",
        "\n",
        "if True:\n",
        "    !pip install git+https://github.com/huggingface/transformers accelerate\n",
        "    !pip install transformers[qwen] --upgrade\n",
        "    !pip install qwen-vl-utils[decord]==0.0.8\n",
        "    !pip install flash-attn\n",
        "    from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "    from qwen_vl_utils import process_vision_info\n",
        "    model, processor = load_qwen_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI9J7L9GuWBs",
        "outputId": "0703fb13-35af-4115-b7df-ea37a7f1402a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2736 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "def split_convert_mathvision_to_json(split):\n",
        "    # Load the dataset\n",
        "    ds = load_dataset(\"MathLLMs/MathVision\")\n",
        "\n",
        "    # Combine train and test splits for reshuffling\n",
        "    train_data = []\n",
        "    test_data = []\n",
        "\n",
        "    test_mini_ids = ds['testmini']['id']\n",
        "    for item in ds[split]:\n",
        "        if item['id'] in test_mini_ids:\n",
        "            test_data.append(item)\n",
        "        else:\n",
        "            train_data.append(item)\n",
        "\n",
        "    # Shuffle and split the data (80% train, 20% test)\n",
        "    # train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Process each split\n",
        "    train_json = process_split(train_data, 'train', generate_solution=True)\n",
        "    test_json = process_split(test_data, 'test', generate_solution=False)\n",
        "\n",
        "    # Save to files\n",
        "    with open(f\"{FOLDER_PATH}/mathvision_train.json\", 'w') as f:\n",
        "        json.dump(train_json, f, indent=2)\n",
        "\n",
        "    with open(f\"{FOLDER_PATH}/mathvision_test.json\", 'w') as f:\n",
        "        json.dump(test_json, f, indent=2)\n",
        "\n",
        "    print(f\"Converted {len(train_json)} entries for train split\")\n",
        "    print(f\"Converted {len(test_json)} entries for test split\")\n",
        "\n",
        "    return train_json, test_json\n",
        "\n",
        "def convert_mathvision_to_json(split):\n",
        "    # Load the dataset\n",
        "    ds = load_dataset(\"MathLLMs/MathVision\")\n",
        "    data_json = process_split(ds[split], split)\n",
        "\n",
        "    # Save to files\n",
        "    with open(f\"{FOLDER_PATH}/mathvision_{split}.json\", 'w') as f:\n",
        "        json.dump(data_json, f, indent=2)\n",
        "\n",
        "    print(f\"Converted {len(data_json)} entries for test split\")\n",
        "\n",
        "    return data_json\n",
        "\n",
        "def find_solution(model_response, item):\n",
        "    # Extract the answer from the response (assuming it's in quotes)\n",
        "    match = re.search(r\"'([^']+)'(\\.|\\s)*$\", model_response)\n",
        "    model_answer = match.group(1) if match else None\n",
        "\n",
        "    # Check if the answer is correct\n",
        "    correct_answer = item.get('answer', '').strip()\n",
        "    is_correct = model_answer and model_answer.strip().lower() == correct_answer.lower()\n",
        "\n",
        "    return model_response if is_correct else f\"\"\"The correct answer is: '{correct_answer}' \"\"\"\n",
        "\n",
        "def process_split(data, split, generate_solution = False):\n",
        "    count_correct_solution = 0\n",
        "    converted_data = []\n",
        "    instruction = \"Answer the following question using a single word or phrase, by considering the image provided.\"\n",
        "    # instruction =\n",
        "    for i, item in enumerate(tqdm(data)):\n",
        "        if i <= 1650:\n",
        "            continue\n",
        "\n",
        "        question_prompt = f\"\"\"Please solve the problem step by step and put your final answer and the end of the solution in single quotes. If it is a multiple choice question, only one letter is allowed in the quotes. \\n {item['question']}\"\"\"\n",
        "\n",
        "        if item.get('options') and len(item['options']) > 0:\n",
        "            question_prompt += f\". Choose from the options {', '.join(item['options'][:-1])}, or {item['options'][-1]}.\"\n",
        "\n",
        "        image_path = f\"{IMG_PATH}/{item.get('image')}\"\n",
        "        if generate_solution:\n",
        "            # First, get the model's answer\n",
        "            # model_response = query_gpt4v(image_path, question_prompt)\n",
        "            model_response = query_qwen(image_path, question_prompt, \"\")\n",
        "\n",
        "            final_solution = find_solution(model_response, item)\n",
        "            if not final_solution.startswith(\"The correct answer is\"):\n",
        "                count_correct_solution += 1\n",
        "            # print(f\"model_response = {model_response}\")\n",
        "            # print(f\"final_solution = {final_solution}\")\n",
        "        else:\n",
        "            final_solution = f\"\"\"The correct answer is: '{item.get('answer', '').strip()}' \"\"\"\n",
        "        # For fine-tuning, include full solution if correct\n",
        "        conversation_entry = {\n",
        "            \"system_prompt\": \"You are a helpful visual assistant that can understand images and answer questions about them accurately and concisely. \" + instruction,\n",
        "            \"image\": item.get(\"image\"),\n",
        "            \"conversations\": [\n",
        "                {\n",
        "                    \"from\": \"human\",\n",
        "                    \"value\": f\"<image>\\n{question_prompt}\"\n",
        "                },\n",
        "                {\n",
        "                    \"from\": \"gpt\",\n",
        "                    \"value\": final_solution\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        converted_data.append(conversation_entry)\n",
        "        if i%50 == 0:\n",
        "            with open(f\"{FOLDER_PATH}/mathvision_{split}.json\", 'w') as f:\n",
        "                json.dump(converted_data, f, indent=2)\n",
        "            print(\"num correct solutions added = \", count_correct_solution)\n",
        "\n",
        "    print(\"FINAL num correct solutions added = \", count_correct_solution)\n",
        "    return converted_data\n",
        "\n",
        "\n",
        "def query_qwen(image_path, prompt, instruction):\n",
        "    messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful visual assistant that can understand images and answer questions about them accurately and concisely.\" + instruction\n",
        "            },\n",
        "             {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image_path},\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Preparation for inference\n",
        "    text = processor.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    # Inference: Generation of the output\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    output_text = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    return output_text[0]\n",
        "\n",
        "\n",
        "\n",
        "# Helper function to query GPT4-V (you'll need to implement this based on your API access)\n",
        "def query_gpt4v(image_path, prompt):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "          base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4-vision-preview\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful visual assistant that can understand images and answer questions about them accurately and concisely.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# Run the conversion\n",
        "train_json, test_json = split_convert_mathvision_to_json('test')\n",
        "\n",
        "# # Convert testmini split\n",
        "# testmini_json = convert_mathvision_to_json('testmini')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset investigation"
      ],
      "metadata": {
        "id": "AZbbyWKuFMYd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBxlTQPPtKFx"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"MathLLMs/MathVision\")['testmini']\n",
        "ds\n",
        "counter = 0\n",
        "for item in tqdm(ds):\n",
        "    if '<image2>' in item['question']:\n",
        "        print(item['id'])\n",
        "        print(item['question'])\n",
        "        print(item['image'])\n",
        "        print('----------')\n",
        "        counter += 1\n",
        "print(counter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating dataframes with solutions if exist"
      ],
      "metadata": {
        "id": "nkbQlpeRFP5l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlNI2yWaSLpL"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "ds = load_dataset(\"MathLLMs/MathVision\")\n",
        "\n",
        "with open(f\"{FOLDER_PATH}/mathvision_train.json\", 'r') as f:\n",
        "    train_json = json.load(f)\n",
        "\n",
        "# Combine train and test splits for reshuffling\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "test_mini_ids = ds['testmini']['id']\n",
        "train_counter = 0\n",
        "with_solution_counter = 0\n",
        "for item in tqdm(ds['test']):\n",
        "    if item['id'] in test_mini_ids:\n",
        "        test_data.append(item)\n",
        "    else:\n",
        "        if train_counter < len(train_json):\n",
        "          json_item = train_json[train_counter]\n",
        "          if (\n",
        "              item['question'] in json_item.get('conversations')[0]['value']\n",
        "              and not json_item['conversations'][1]['value'].startswith('The correct answer is:')\n",
        "          ):\n",
        "              item['solution'] = json_item['conversations'][1]['value']\n",
        "              with_solution_counter += 1\n",
        "        train_data.append(item)\n",
        "        train_counter += 1\n",
        "\n",
        "\n",
        "print(f'There are {with_solution_counter} questions in the train split with solutions')\n",
        "pd.DataFrame(train_data).to_csv(f\"{FOLDER_PATH}/mathvision_train.csv\", index=False)\n",
        "pd.DataFrame(test_data).to_csv(f\"{FOLDER_PATH}/mathvision_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging two json files with dictionaries"
      ],
      "metadata": {
        "id": "3KC6O7gxFThc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "filepath_1 = \"/content/drive/MyDrive/reasoning_multimodal_LLMs/example_data/mathvision_train_1to1650.json\"\n",
        "filepath_2 = \"/content/drive/MyDrive/reasoning_multimodal_LLMs/example_data/mathvision_train.json\"\n",
        "# Load first JSON file\n",
        "with open(filepath_1, \"r\") as file:\n",
        "    data1 = json.load(file)\n",
        "\n",
        "# Load second JSON file\n",
        "with open(filepath_2, \"r\") as file:\n",
        "    data2 = json.load(file)\n",
        "\n",
        "# Merge dictionaries (data2 overwrites data1 in case of key conflicts)\n",
        "merged_data = {**data1, **data2}\n",
        "\n",
        "# Save the merged JSON\n",
        "with open(filepath_2, \"w\") as file:\n",
        "    json.dump(merged_data, file, indent=4)\n",
        "\n",
        "print(\"Merged JSON saved successfully!\")\n"
      ],
      "metadata": {
        "id": "pWD83jYkFJNm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1 (main, Dec  3 2024, 17:59:52) [Clang 16.0.0 (clang-1600.0.26.4)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a705a04fb367ec22af69d2ff9887b181893499ca251cf5bc866022978b76dc8e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f653d92fcd94433ab7a8fc598426d87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb1984d325714c0e99d67a31bdfeead4",
              "IPY_MODEL_f95e1b23a15549b480f39b63b8744709",
              "IPY_MODEL_9031cc6f67df49f3912febfa658f215d"
            ],
            "layout": "IPY_MODEL_ef8f08019c5f4ac3916013d99eaecb0b"
          }
        },
        "bb1984d325714c0e99d67a31bdfeead4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adc6e52810bf49238a7905292ded866b",
            "placeholder": "​",
            "style": "IPY_MODEL_18ee6ea0c62b42a48b8d108aff4d578a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f95e1b23a15549b480f39b63b8744709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df74531141d146eca0ecd399ba89b0c4",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e54eb2bfb090448ba77361b7315b6f71",
            "value": 5
          }
        },
        "9031cc6f67df49f3912febfa658f215d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bc7f36b4213469ca12dc449880169dd",
            "placeholder": "​",
            "style": "IPY_MODEL_cddaea09cf624068b9a104a4ef65dbb5",
            "value": " 5/5 [00:07&lt;00:00,  1.35s/it]"
          }
        },
        "ef8f08019c5f4ac3916013d99eaecb0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adc6e52810bf49238a7905292ded866b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18ee6ea0c62b42a48b8d108aff4d578a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df74531141d146eca0ecd399ba89b0c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e54eb2bfb090448ba77361b7315b6f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7bc7f36b4213469ca12dc449880169dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cddaea09cf624068b9a104a4ef65dbb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}