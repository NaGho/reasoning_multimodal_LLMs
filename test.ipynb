{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MathVisionEvaluator:\n",
    "    def __init__(self, dataset_name=\"MathLLMs/MathVision\", models=None):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with the dataset and models\n",
    "        \n",
    "        :param dataset_name: Name of the dataset to evaluate\n",
    "        :param models: Dictionary of models to test {'model_name': {'model': model, 'tokenizer': tokenizer}}\n",
    "        \"\"\"\n",
    "        # Load dataset\n",
    "        self.dataset = load_dataset(dataset_name)\n",
    "        \n",
    "        # Default model configurations if not provided\n",
    "        if models is None:\n",
    "            models = {\n",
    "                'llava1.5': self._load_llava_model(),\n",
    "                'qwen2_vl': self._load_qwen2_model()\n",
    "            }\n",
    "        self.models = models\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        self.results = {model_name: {'zero_shot': [], 'few_shot': []} \n",
    "                        for model_name in models.keys()}\n",
    "    \n",
    "    def _load_llava_model(self):\n",
    "        \"\"\"Load Llava 1.5 model and tokenizer\"\"\"\n",
    "        model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, \n",
    "            torch_dtype=torch.float16, \n",
    "            device_map='auto'\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        return {'model': model, 'tokenizer': tokenizer}\n",
    "    \n",
    "    def _load_qwen2_model(self):\n",
    "        \"\"\"Load Qwen2 VL model and tokenizer\"\"\"\n",
    "        model_id = \"Qwen/Qwen-VL\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, \n",
    "            torch_dtype=torch.float16, \n",
    "            device_map='auto'\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        return {'model': model, 'tokenizer': tokenizer}\n",
    "    \n",
    "    def _generate_prompt(self, example, few_shot=False):\n",
    "        \"\"\"\n",
    "        Generate prompts for the models\n",
    "        \n",
    "        :param example: Single dataset example\n",
    "        :param few_shot: Whether to use few-shot prompting\n",
    "        :return: Prompt string\n",
    "        \"\"\"\n",
    "        base_prompt = f\"Question: {example['question']}\\nPlease solve this math problem step by step.\"\n",
    "        \n",
    "        if few_shot:\n",
    "            # Add 1-2 example solutions to provide context\n",
    "            few_shot_examples = self.dataset['train'][:2]\n",
    "            base_prompt = (\n",
    "                \"Here are a couple of example solutions:\\n\" +\n",
    "                \"\\n\".join([\n",
    "                    f\"Example {i+1} Question: {ex['question']}\\nSolution: {ex['answer']}\"\n",
    "                    for i, ex in enumerate(few_shot_examples)\n",
    "                ]) + \n",
    "                f\"\\n\\nNow solve this problem:\\n{base_prompt}\"\n",
    "            )\n",
    "        \n",
    "        return base_prompt\n",
    "    \n",
    "    def evaluate(self, prompt_types=['zero_shot', 'few_shot']):\n",
    "        \"\"\"\n",
    "        Evaluate models on the dataset\n",
    "        \n",
    "        :param prompt_types: Types of prompting to use\n",
    "        \"\"\"\n",
    "        for split in ['test', 'validation']:\n",
    "            for model_name, model_config in self.models.items():\n",
    "                for prompt_type in prompt_types:\n",
    "                    model, tokenizer = model_config['model'], model_config['tokenizer']\n",
    "                    \n",
    "                    model_results = []\n",
    "                    for example in tqdm(self.dataset[split], \n",
    "                                        desc=f\"Evaluating {model_name} - {prompt_type}\"):\n",
    "                        # Generate prompt\n",
    "                        prompt = self._generate_prompt(\n",
    "                            example, \n",
    "                            few_shot=(prompt_type == 'few_shot')\n",
    "                        )\n",
    "                        \n",
    "                        # Generate model response\n",
    "                        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "                        outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "                        \n",
    "                        # Decode and process response\n",
    "                        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                        \n",
    "                        # Compare with ground truth\n",
    "                        model_results.append({\n",
    "                            'question': example['question'],\n",
    "                            'ground_truth': example['answer'],\n",
    "                            'model_prediction': generated_text,\n",
    "                            'correct': self._check_answer(example['answer'], generated_text)\n",
    "                        })\n",
    "                    \n",
    "                    # Store results\n",
    "                    self.results[model_name][prompt_type] = model_results\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def _check_answer(self, ground_truth, prediction):\n",
    "        \"\"\"\n",
    "        Basic answer checking method\n",
    "        Can be enhanced with more sophisticated comparison\n",
    "        \n",
    "        :param ground_truth: Correct answer from dataset\n",
    "        :param prediction: Model's generated answer\n",
    "        :return: Boolean indicating correctness\n",
    "        \"\"\"\n",
    "        # Simple string matching (can be replaced with more advanced techniques)\n",
    "        return ground_truth.lower() in prediction.lower()\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"\n",
    "        Print summary of evaluation results\n",
    "        \"\"\"\n",
    "        for model_name, prompt_types in self.results.items():\n",
    "            print(f\"\\n{model_name.upper()} Model Performance:\")\n",
    "            for prompt_type, results in prompt_types.items():\n",
    "                correct_count = sum(res['correct'] for res in results)\n",
    "                total_count = len(results)\n",
    "                accuracy = correct_count / total_count * 100\n",
    "                \n",
    "                print(f\"{prompt_type.replace('_', ' ').title()} Accuracy: {accuracy:.2f}%\")\n",
    "                print(f\"Correct Predictions: {correct_count}/{total_count}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = MathVisionEvaluator()\n",
    "    evaluator.evaluate()\n",
    "    evaluator.print_summary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
