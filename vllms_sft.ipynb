{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/zjysteven/lmms-finetune.git\n",
        "import sys\n",
        "sys.path.append('/content/lmms-finetune')\n"
      ],
      "metadata": {
        "id": "nq8Fz3QLE7hV",
        "outputId": "8911f0ee-1ef2-4807-c2cf-d8d29b63f72c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'lmms-finetune' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers==4.45.2\n",
        "# !pip install accelerate\n",
        "# !pip install deepspeed==0.14.4\n",
        "# !pip install av\n",
        "# !pip install peft\n",
        "# # !pip install --force-reinstall -U bitsandbytes\n",
        "# !pip uninstall bitsandbytes -y\n",
        "# !pip install --no-cache-dir bitsandbytes\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_PROJECT\"]= \"lmms-ft\"\n",
        "from dataclasses import asdict\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import List, Optional\n",
        "import yaml\n",
        "\n",
        "from accelerate.utils import DistributedType\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import Trainer, deepspeed\n",
        "# import deepspeed\n",
        "\n",
        "from arguments import ModelArguments, DataArguments, TrainingArguments, LoraArguments\n",
        "from collators import COLLATORS\n",
        "from loaders import LOADERS\n",
        "from supported_models import MODULE_KEYWORDS\n",
        "from utils import (\n",
        "    rank0_print, find_all_linear_names, safe_save_model_for_hf_trainer,\n",
        "    get_peft_state_maybe_zero_3, TrainerWithCustomSampler\n",
        ")\n",
        "\n",
        "DATA_FOLDER = \"/content/lmms-finetune/example_data\"\n",
        "# def train():\n",
        "parser = transformers.HfArgumentParser(\n",
        "    (ModelArguments, DataArguments, TrainingArguments, LoraArguments)\n",
        ")\n",
        "\n",
        "parser.parse_args_into_dataclasses(args=[\"--output_dir\", \"./outputs\"])\n",
        "# Define default arguments as a list\n",
        "default_args = [\n",
        "    \"--output_dir\", \"./outputs\",\n",
        "]\n",
        "model_args, data_args, training_args, lora_args = parser.parse_args_into_dataclasses(default_args)\n",
        "data_args.data_path = DATA_FOLDER + \"/single_image.json\"\n",
        "data_args.image_folder = DATA_FOLDER + \"/images\"\n",
        "# data_args.video_folder = DATA_FOLDER + \"/videos\"\n",
        "\n",
        "\n",
        "# dumping arguments\n",
        "output_dir = getattr(training_args, 'output_dir', None)\n",
        "assert output_dir is not None, \"output_dir is required\"\n",
        "args_dir = Path(output_dir) / \"arguments\"\n",
        "args_dir.mkdir(parents=True, exist_ok=True)\n",
        "yaml.dump(asdict(model_args), open(args_dir / \"model.yaml\", \"w\"))\n",
        "yaml.dump(asdict(data_args), open(args_dir / \"data.yaml\", \"w\"))\n",
        "yaml.dump(asdict(training_args), open(args_dir / \"training.yaml\", \"w\"))\n",
        "yaml.dump(asdict(lora_args), open(args_dir / \"lora.yaml\", \"w\"))\n",
        "\n",
        "compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n",
        "if getattr(training_args, 'deepspeed', None) and getattr(lora_args, 'q_lora', False):\n",
        "    training_args.distributed_state.distributed_type = DistributedType.DEEPSPEED\n",
        "\n",
        "device_map = None\n",
        "if lora_args.q_lora:\n",
        "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)} if int(os.environ.get(\"WORLD_SIZE\", 1)) != 1 else None\n",
        "    if len(training_args.fsdp) > 0 or deepspeed.is_deepspeed_zero3_enabled():\n",
        "        raise ValueError(\"FSDP or ZeRO3 are not incompatible with QLoRA.\")\n",
        "\n",
        "# llm quantization config (for q-lora)\n",
        "bnb_config = None\n",
        "if lora_args.use_lora and lora_args.q_lora:\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    rank0_print(\"Quantization for LLM enabled...\")\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "    )\n",
        "\n",
        "# load model, tokenizer, processor\n",
        "rank0_print(\"Loading model, tokenizer, processor...\")\n",
        "print(\"model_args = \", model_args)\n",
        "loader = LOADERS[model_args.model_family_id](\n",
        "    model_hf_path=model_args.model_hf_path,\n",
        "    model_local_path=model_args.model_local_path,\n",
        "    compute_dtype=compute_dtype,\n",
        "    bnb_config=bnb_config,\n",
        "    use_flash_attn=training_args.use_flash_attn,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model, tokenizer, processor, config = loader.load()\n",
        "tokenizer.model_max_length = training_args.model_max_length\n",
        "\n",
        "if training_args.gradient_checkpointing:\n",
        "    model.enable_input_require_grads()\n",
        "\n",
        "# freeze certain params\n",
        "vision_encoder_keys = MODULE_KEYWORDS[model_args.model_family_id][\"vision_encoder\"]\n",
        "if not training_args.train_vision_encoder:\n",
        "    rank0_print(f\"Vision encoder is freezed... including:\")\n",
        "    for module in vision_encoder_keys:\n",
        "        rank0_print(f\"\\t{module}\")\n",
        "        eval(f\"model.{module}\").requires_grad_(False)\n",
        "\n",
        "vision_projector_keys = MODULE_KEYWORDS[model_args.model_family_id][\"vision_projector\"]\n",
        "if not training_args.train_vision_projector:\n",
        "    rank0_print(f\"Vision projector is freezed... including:\")\n",
        "    for module in vision_projector_keys:\n",
        "        rank0_print(f\"\\t{module}\")\n",
        "        eval(f\"model.{module}\").requires_grad_(False)\n",
        "\n",
        "# other components preparation (e.g., image_newline, vision_resampler)\n",
        "# we will just freeze these\n",
        "if \"others\" in MODULE_KEYWORDS[model_args.model_family_id]:\n",
        "    rank0_print(f\"Other multimodal component is freezed... including:\")\n",
        "    for other_key in MODULE_KEYWORDS[model_args.model_family_id][\"others\"]:\n",
        "        rank0_print(f\"\\t{other_key}\")\n",
        "        eval(f\"model.{other_key}\").requires_grad_(False)\n",
        "\n",
        "# lora preparation\n",
        "llm_keys = MODULE_KEYWORDS[model_args.model_family_id][\"llm\"]\n",
        "if not (lora_args.use_lora or (training_args.train_vision_encoder and lora_args.use_vision_lora)):\n",
        "    rank0_print(\"No LoRA enabled...\")\n",
        "else:\n",
        "    named_modules = {n: m for n, m in model.named_modules()}\n",
        "    lora_modules = []\n",
        "    full_modules = []\n",
        "\n",
        "    if training_args.train_vision_encoder and lora_args.use_vision_lora:\n",
        "        rank0_print(\"LoRA for vision encoder enabled...\")\n",
        "        lora_modules.extend(find_all_linear_names(named_modules, vision_encoder_keys))\n",
        "    elif training_args.train_vision_encoder:\n",
        "        rank0_print(\"Vision encoder will be fully trained...\")\n",
        "        full_modules.extend(vision_encoder_keys)\n",
        "\n",
        "    if lora_args.use_lora:\n",
        "        rank0_print(\"LoRA for LLM enabled...\")\n",
        "        lora_modules.extend(find_all_linear_names(named_modules, llm_keys))\n",
        "    else:\n",
        "        rank0_print(\"LLM will be fully trained...\")\n",
        "        full_modules.extend(llm_keys)\n",
        "\n",
        "    if training_args.train_vision_projector:\n",
        "        rank0_print(\"Vision projector will be fully trained...\")\n",
        "        full_modules.extend(vision_projector_keys)\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=lora_args.lora_r,\n",
        "        lora_alpha=lora_args.lora_alpha,\n",
        "        target_modules=lora_modules,\n",
        "        modules_to_save=full_modules,\n",
        "        lora_dropout=lora_args.lora_dropout,\n",
        "        bias=lora_args.lora_bias,\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    if lora_args.q_lora:\n",
        "        model = prepare_model_for_kbit_training(\n",
        "            model, use_gradient_checkpointing=training_args.gradient_checkpointing\n",
        "        )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "# print trainable parameters for inspection\n",
        "rank0_print(\"Trainable parameters:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        rank0_print(f\"\\t{name}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kkj4BM_Z6znC",
        "outputId": "c9276a9b-0940-46e9-c619-d122fa9b8f1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "03d887ebfce84e0e953e19cea5071c77",
            "b3ba540a505741cfbfebfb2242d13c7b",
            "37c7eb87eee24bd493c455693d92eb31",
            "ad240c74f5a94596a15edd070fdf64fc",
            "300acfbd0bcc4a2c8604887e9aa16a3e",
            "83f32da81afb4e81a7e6b30a1082bb7d",
            "4721b801b396436496a3b01486be2176",
            "8c89a1de96fe4dc693b276a7140aa79b",
            "26f05d1cfd84409fa196e9074e50cb3c",
            "6598a67867f4471491e2db4107402102",
            "cb53a4c5a31d4b618c3047ddb8640dc5"
          ]
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_args =  DataArguments(data_path='/content/llms-finetune/example_data/single_image.json', eval_data_path=None, image_folder='/content/llms-finetune/example_data/images', video_folder=None, num_frames=8, user_key='human', assistant_key='gpt')\n",
            "model_args =  ModelArguments(model_id='llava-1.5-7b', model_local_path='llava-hf/llava-1.5-7b-hf')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03d887ebfce84e0e953e19cea5071c77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "from datasets import LazySupervisedDataset\n",
        "# import json\n",
        "# print(os.getcwd())\n",
        "# print(json.load(open('/content/lmms-finetune/example_data/single_image.json', \"r\")))\n",
        "rank0_print(\"Loading data...\")\n",
        "train_dataset = LazySupervisedDataset(\n",
        "    data_path=data_args.data_path,\n",
        "    image_folder=data_args.image_folder,\n",
        "    video_folder=data_args.video_folder,\n",
        "    num_frames=data_args.num_frames,\n",
        "    model_family_id=model_args.model_family_id,\n",
        "    user_key=data_args.user_key,\n",
        "    assistant_key=data_args.assistant_key\n",
        ")\n",
        "if data_args.eval_data_path:\n",
        "    eval_dataset = LazySupervisedDataset(\n",
        "        data_path=data_args.eval_data_path,\n",
        "        image_folder=data_args.image_folder,\n",
        "        video_folder=data_args.video_folder,\n",
        "        num_frames=data_args.num_frames,\n",
        "        model_family_id=model_args.model_family_id,\n",
        "        user_key=data_args.user_key,\n",
        "        assistant_key=data_args.assistant_key\n",
        "    )\n",
        "else:\n",
        "    eval_dataset = None\n",
        "    training_args.eval_strategy = \"no\"\n",
        "\n",
        "# data collator\n",
        "data_collator = COLLATORS[model_args.model_family_id](\n",
        "    config=config,\n",
        "    tokenizer=tokenizer,\n",
        "    processor=processor,\n",
        "    mask_question_tokens=training_args.mask_question_tokens\n",
        ")\n",
        "\n",
        "# trainer\n",
        "trainer = TrainerWithCustomSampler(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_state()\n",
        "\n",
        "safe_save_model_for_hf_trainer(trainer=trainer, output_dir=output_dir)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     train()"
      ],
      "metadata": {
        "id": "TIGOccYyIhuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "d8b83d43-8a75-4e6e-ca49-981eb9ae3212"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/llms-finetune/example_data/single_image.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2bd87ff85dbf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(json.load(open('/content/lmms-finetune/example_data/single_image.json', \"r\")))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrank0_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m train_dataset = LazySupervisedDataset(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimage_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/lmms-finetune/datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, model_family_id, image_folder, video_folder, num_frames, user_key, assistant_key)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLazySupervisedDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_data_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/llms-finetune/example_data/single_image.json'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03d887ebfce84e0e953e19cea5071c77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3ba540a505741cfbfebfb2242d13c7b",
              "IPY_MODEL_37c7eb87eee24bd493c455693d92eb31",
              "IPY_MODEL_ad240c74f5a94596a15edd070fdf64fc"
            ],
            "layout": "IPY_MODEL_300acfbd0bcc4a2c8604887e9aa16a3e"
          }
        },
        "b3ba540a505741cfbfebfb2242d13c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83f32da81afb4e81a7e6b30a1082bb7d",
            "placeholder": "​",
            "style": "IPY_MODEL_4721b801b396436496a3b01486be2176",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "37c7eb87eee24bd493c455693d92eb31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c89a1de96fe4dc693b276a7140aa79b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26f05d1cfd84409fa196e9074e50cb3c",
            "value": 3
          }
        },
        "ad240c74f5a94596a15edd070fdf64fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6598a67867f4471491e2db4107402102",
            "placeholder": "​",
            "style": "IPY_MODEL_cb53a4c5a31d4b618c3047ddb8640dc5",
            "value": " 3/3 [01:14&lt;00:00, 24.28s/it]"
          }
        },
        "300acfbd0bcc4a2c8604887e9aa16a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f32da81afb4e81a7e6b30a1082bb7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4721b801b396436496a3b01486be2176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c89a1de96fe4dc693b276a7140aa79b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26f05d1cfd84409fa196e9074e50cb3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6598a67867f4471491e2db4107402102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb53a4c5a31d4b618c3047ddb8640dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}