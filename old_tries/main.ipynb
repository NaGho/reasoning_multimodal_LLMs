{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoProcessor, \n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    LlavaForConditionalGeneration,\n",
    "    AutoModelForVision2Seq\n",
    ")\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "class VisionLanguageProcessor:\n",
    "    def __init__(self, model_type=\"qwen\", optimize_memory=True):\n",
    "        \"\"\"\n",
    "        Initialize the vision-language processor with specified model type.\n",
    "        Args:\n",
    "            model_type (str): One of \"llava\" or \"qwen\"\n",
    "            optimize_memory (bool): Whether to apply memory optimizations\n",
    "        \"\"\"\n",
    "        self.MODEL_TYPE = model_type.lower()\n",
    "        self.device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        self.use_half = False\n",
    "        \n",
    "        model_configs = {\n",
    "            \"llava\": {\n",
    "                \"name\": \"llava-hf/llava-1.5-7b-hf\",\n",
    "                \"model_class\": LlavaForConditionalGeneration,\n",
    "            },\n",
    "            \"qwen\": {\n",
    "                \"name\": \"Qwen/Qwen2-VL-7B\",\n",
    "                \"model_class\": AutoModelForVision2Seq,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if self.MODEL_TYPE not in model_configs:\n",
    "            raise ValueError(f\"Invalid model type. Choose from {list(model_configs.keys())}\")\n",
    "            \n",
    "        config = model_configs[self.MODEL_TYPE]\n",
    "        \n",
    "        try:\n",
    "            print(f\"Loading {self.MODEL_TYPE} model...\")\n",
    "            \n",
    "            if optimize_memory:\n",
    "                torch.backends.cudnn.benchmark = True\n",
    "                if hasattr(torch.backends, 'cudnn'):\n",
    "                    torch.backends.cudnn.deterministic = False\n",
    "            \n",
    "            # Load components in correct order\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                config[\"name\"],\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            self.processor = AutoProcessor.from_pretrained(\n",
    "                config[\"name\"],\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            self.model = config[\"model_class\"].from_pretrained(\n",
    "                config[\"name\"],\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "                device_map='auto'\n",
    "            )\n",
    "            \n",
    "            if hasattr(self.model, \"gradient_checkpointing_enable\"):\n",
    "                self.model.gradient_checkpointing_enable()\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading {self.MODEL_TYPE} model: {e}\")\n",
    "\n",
    "    def load_image(self, image_url):\n",
    "        \"\"\"Load and preprocess image from URL.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(image_url, stream=True, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return Image.open(response.raw).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing image: {e}\")\n",
    "\n",
    "    def process(self, image_url, user_prompt, max_length=200):\n",
    "        \"\"\"Process image and text prompt with optimized settings.\"\"\"\n",
    "        try:\n",
    "            # Load raw image\n",
    "            image = self.load_image(image_url)\n",
    "            \n",
    "            if self.MODEL_TYPE == \"llava\":\n",
    "                return self._process_llava(image, user_prompt, max_length)\n",
    "            else:  # qwen\n",
    "                return self._process_qwen(image, user_prompt, max_length)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error during processing: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _process_llava(self, image, user_prompt, max_length):\n",
    "        \"\"\"Process using LLaVA model with optimized generation parameters.\"\"\"\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=user_prompt,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            output_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                min_length=30,\n",
    "                num_beams=2,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_k=40,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "        \n",
    "        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    def _process_qwen(self, image, user_prompt, max_length):\n",
    "        \"\"\"Process using Qwen2-VL model with optimized generation parameters.\"\"\"\n",
    "        prompt = f\"<image>\\n{user_prompt}\"\n",
    "        \n",
    "        # Process image and text together\n",
    "        inputs = self.processor(\n",
    "            text=prompt,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            output_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                min_new_tokens=30,\n",
    "                num_beams=2,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_k=40,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "\n",
    "def main():\n",
    "    # Test with a single model\n",
    "    processor = VisionLanguageProcessor(model_type=\"qwen\", optimize_memory=True)\n",
    "    \n",
    "    image_url = \"https://www.thespruceeats.com/thmb/s11oj7aiRC0zcjIXuu80NmT-L4o=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/SES-basic-meat-lasagna-recipe-2097886-hero-01-cdd28f5b4aa940faa193e39a1629f89a.jpg\"\n",
    "    user_prompt = \"Describe this food in detail and suggest a drink pairing.\"\n",
    "    \n",
    "    try:\n",
    "        output_text = processor.process(image_url, user_prompt)\n",
    "        if output_text:\n",
    "            print(\"Generated Text:\", output_text)\n",
    "        else:\n",
    "            print(\"Error in processing.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d5f51d25b75cc64eefc1de00c1ec182f18cbd54418fd3992210cc7222ab69c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
