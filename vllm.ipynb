{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuJtYzclavpw",
        "outputId": "80c0da69-797d-44a0-a14a-2035258d8862",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qwen_vl_utils\n",
            "  Downloading qwen_vl_utils-0.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting av (from qwen_vl_utils)\n",
            "  Downloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from qwen_vl_utils) (24.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from qwen_vl_utils) (11.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from qwen_vl_utils) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->qwen_vl_utils) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->qwen_vl_utils) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->qwen_vl_utils) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->qwen_vl_utils) (2024.12.14)\n",
            "Downloading qwen_vl_utils-0.0.8-py3-none-any.whl (5.9 kB)\n",
            "Downloading av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av, qwen_vl_utils\n",
            "Successfully installed av-14.0.1 qwen_vl_utils-0.0.8\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.0\n",
            "Collecting trl\n",
            "  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.2.1)\n",
            "Requirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from trl) (3.2.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.47.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.27.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.11.10)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (3.0.2)\n",
            "Downloading trl-0.13.0-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trl\n",
            "Successfully installed trl-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install qwen_vl_utils\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install -U bitsandbytes\n",
        "!pip install trl\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    Qwen2VLForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        "    Qwen2VLProcessor,\n",
        "    AutoTokenizer,\n",
        "    AutoImageProcessor\n",
        ")\n",
        "from trl import SFTConfig\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from datasets import load_dataset#, Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "import wandb\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import gc\n",
        "import time\n",
        "from typing import Dict, List\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "compute = True\n",
        "\n",
        "# Check device compatibility\n",
        "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print('device = ', device)\n",
        "\n",
        "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "\n",
        "img_folder_path = \"/content/drive/MyDrive/MATH-V-main\"\n",
        "\n",
        "dataset_name = \"MathLLMs/MathVision\"\n",
        "\n",
        "file_name = f\"data/output/{dataset_name.split('/')[-1]}_{model_name.split('/')[-1]}.csv\"\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "data_list = load_dataset(dataset_name, split='test')\n",
        "\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_size = int(0.7 * len(data_list))\n",
        "val_size = int(0.1 * len(data_list))\n",
        "test_size = len(data_list) - train_size - val_size\n",
        "data_list = data_list.shuffle(seed=42)  # Shuffle the dataset for randomness\n",
        "\n",
        "train_data = data_list.select(range(train_size))\n",
        "val_data = data_list.select(range(train_size, train_size + val_size))\n",
        "test_data = data_list.select(range(train_size+val_size, len(data_list)))\n"
      ],
      "metadata": {
        "id": "Imbw23lIfOWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_message(inputs):\n",
        "    messages = []\n",
        "    for input in inputs:\n",
        "        prompt = input[\"question\"]\n",
        "        image_path = f\"{img_folder_path}/{input['image']}\"\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"image\", \"image\": image_path}, {\"type\": \"text\", \"text\": prompt}]\n",
        "        })\n",
        "\n",
        "    return messages\n",
        "\n",
        "def generate_embedding(model, processor, sample):\n",
        "    messages = generate_message([sample])\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    # Move inputs to the device\n",
        "    inputs = inputs.to(device)\n",
        "    return inputs\n",
        "\n",
        "def generate_text_from_sample(model, processor, sample, max_new_tokens=1024):\n",
        "    inputs = generate_embedding(model, processor, sample)\n",
        "\n",
        "    # Move model to the device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
        "\n",
        "    # Trim the generated ids to remove the input ids\n",
        "    trimmed_generated_ids = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)]\n",
        "\n",
        "    output_text = processor.batch_decode(\n",
        "        trimmed_generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "    return output_text\n"
      ],
      "metadata": {
        "id": "6soYmdYVNdyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "try:\n",
        "  model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "      f\"{img_folder_path}/{model_name}\",\n",
        "      device_map=\"auto\",\n",
        "      torch_dtype=torch.bfloat16,\n",
        "      # quantization_config=bnb_config\n",
        "  )\n",
        "  model.save_pretrained(f\"{img_folder_path}/{model_name}\")\n",
        "except:\n",
        "  model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "      model_name,\n",
        "      device_map=\"auto\",\n",
        "      torch_dtype=torch.bfloat16,\n",
        "      # quantization_config=bnb_config\n",
        "  )\n",
        "processor = Qwen2VLProcessor.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    r=8,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply PEFT model adaptation\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "5GsacJOFA5vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MessageDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return generate_message([self.data[idx]])[0]\n",
        "\n",
        "\n",
        "# Create a data collator to encode text and image pairs\n",
        "def collate_fn(examples):\n",
        "    # Get the texts and images, and apply the chat template\n",
        "    # message_list = generate_message(examples)\n",
        "    # print('message_list = ', message_list)\n",
        "    texts = [\n",
        "        processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in examples\n",
        "    ]  # Prepare texts for processing\n",
        "    # print('texts = ', texts)\n",
        "    image_inputs = [process_vision_info([msg])[0] for msg in examples]  # Process the images to extract inputs\n",
        "    # print('image_inputs = ', image_inputs)\n",
        "    # Tokenize the texts and process the images\n",
        "    batch = processor(\n",
        "        text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n",
        "    )  # Encode texts and images into tensors\n",
        "\n",
        "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
        "    labels = batch[\"input_ids\"].clone()  # Clone input IDs for labels\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n",
        "\n",
        "    # Ignore the image token index in the loss computation (model specific)\n",
        "    if isinstance(processor, Qwen2VLProcessor):  # Check if the processor is Qwen2VLProcessor\n",
        "        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n",
        "    else:\n",
        "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n",
        "\n",
        "    # Mask image token IDs in the labels\n",
        "    for image_token_id in image_tokens:\n",
        "        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
        "\n",
        "    batch[\"labels\"] = labels  # Add labels to the batch\n",
        "\n",
        "    return batch  # Return the prepared batch\n"
      ],
      "metadata": {
        "id": "wnmNdD-wgUiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, tokenizer, and image processor with trust_remote_code=True\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# Set output directory\n",
        "output_dir = f\"./fine_tuned_{model_name}\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# test training with CPU\n",
        "if False:\n",
        "  # Define training arguments\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=output_dir,\n",
        "      num_train_epochs=3,\n",
        "      per_device_train_batch_size=2,  # Reduced batch size\n",
        "      per_device_eval_batch_size=2,   # Reduced batch size\n",
        "      warmup_steps=500,\n",
        "      weight_decay=0.01,\n",
        "      logging_dir=os.path.join(output_dir, 'logs'),\n",
        "      logging_steps=100,\n",
        "      evaluation_strategy=\"steps\",\n",
        "      eval_steps=500,\n",
        "      save_steps=1000,\n",
        "      learning_rate=2e-5,\n",
        "      fp16=True,\n",
        "      gradient_accumulation_steps=8,  # Increased gradient accumulation\n",
        "      save_total_limit=3,\n",
        "      remove_unused_columns=False,\n",
        "      dataloader_num_workers=2\n",
        "  )\n",
        "\n",
        "  # Initialize trainer\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=MessageDataset(train_data),\n",
        "      eval_dataset=MessageDataset(val_data),\n",
        "      data_collator=collate_fn,\n",
        "  )\n",
        "\n",
        "  # Start training\n",
        "  trainer.train()"
      ],
      "metadata": {
        "id": "kWf-W_PeOdyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure training arguments\n",
        "training_args = SFTConfig(\n",
        "    output_dir=output_dir,  # Directory to save the model\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    per_device_train_batch_size=4,  # Batch size for training\n",
        "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
        "    gradient_accumulation_steps=8,  # Steps to accumulate gradients\n",
        "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
        "    # Optimizer and scheduler settings\n",
        "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
        "    learning_rate=2e-4,  # Learning rate for training\n",
        "    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
        "    # Logging and evaluation\n",
        "    logging_steps=10,  # Steps interval for logging\n",
        "    eval_steps=10,  # Steps interval for evaluation\n",
        "    eval_strategy=\"steps\",  # Strategy for evaluation\n",
        "    save_strategy=\"steps\",  # Strategy for saving the model\n",
        "    save_steps=20,  # Steps interval for saving\n",
        "    metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n",
        "    greater_is_better=False,  # Whether higher metric values are better\n",
        "    load_best_model_at_end=True,  # Load the best model after training\n",
        "    # Mixed precision and gradient settings\n",
        "    bf16=True,  # Use bfloat16 precision\n",
        "    tf32=True,  # Use TensorFloat-32 precision\n",
        "    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
        "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
        "    # Hub and reporting\n",
        "    push_to_hub=False,  # Whether to push model to Hugging Face Hub\n",
        "    report_to=\"wandb\",  # Reporting tool for tracking metrics\n",
        "    # Gradient checkpointing settings\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
        "    # Dataset configuration\n",
        "    dataset_text_field=\"\",  # Text field in dataset\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n",
        "    # max_seq_length=1024  # Maximum sequence length for input\n",
        ")\n",
        "\n",
        "training_args.remove_unused_columns = False  # Keep unused columns in dataset\n",
        "\n",
        "\n",
        "wandb.init(\n",
        "    project=\"qwen2-7b-instruct-trl-sft-ChartQA\",  # change this\n",
        "    name=\"qwen2-7b-instruct-trl-sft-ChartQA\",  # change this\n",
        "    config=training_args,\n",
        ")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "# # Create datasets\n",
        "# math_train_data = MathVQADataset(train_data, tokenizer, image_processor)\n",
        "# math_val_data = MathVQADataset(val_data, tokenizer, image_processor)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=MessageDataset(train_data), #math_train_data, #\n",
        "    eval_dataset=MessageDataset(val_data), # math_val_data, #\n",
        "    data_collator=collate_fn,\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=processor.tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(training_args.output_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "XTRm0QKvRxs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clear_memory():\n",
        "    # Delete variables if they exist in the current global scope\n",
        "    if \"inputs\" in globals():\n",
        "        del globals()[\"inputs\"]\n",
        "    if \"model\" in globals():\n",
        "        del globals()[\"model\"]\n",
        "    if \"processor\" in globals():\n",
        "        del globals()[\"processor\"]\n",
        "    if \"trainer\" in globals():\n",
        "        del globals()[\"trainer\"]\n",
        "    if \"peft_model\" in globals():\n",
        "        del globals()[\"peft_model\"]\n",
        "    if \"bnb_config\" in globals():\n",
        "        del globals()[\"bnb_config\"]\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Garbage collection and clearing CUDA memory\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "\n",
        "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "\n",
        "clear_memory()"
      ],
      "metadata": {
        "id": "qQa5ZcKkUJ0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adapter_path = \"sergiopaniego/qwen2-7b-instruct-trl-sft-ChartQA\"\n",
        "model.load_adapter(adapter_path)"
      ],
      "metadata": {
        "id": "eARpfPjqULot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "\n",
        "# Load the model\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float32, #if device == \"cpu\" else torch.bfloat16,\n",
        "    device_map=None\n",
        ")\n",
        "\n",
        "# Initialize processor\n",
        "min_pixels = 256 * 28 * 28\n",
        "max_pixels = 1280 * 28 * 28\n",
        "processor = AutoProcessor.from_pretrained(model_name, min_pixels=min_pixels, max_pixels=max_pixels)\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, input in enumerate(tqdm(test_data)):\n",
        "    output_text = generate_text_from_sample(model, processor, input)\n",
        "\n",
        "    # Store results\n",
        "    results.append(input | {\n",
        "        \"generated_text\": output_text\n",
        "    })\n",
        "    # if i > 4:\n",
        "    #   break\n",
        "\n",
        "\n",
        "pd.DataFrame(results).to_csv(f\"{img_folder_path}/test_generated_answer.csv\")"
      ],
      "metadata": {
        "id": "MK3w-7def3Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MathVQADataset(Dataset):\n",
        "    def __init__(self, data: List[Dict], tokenizer, image_processor, max_length=512, image_size=(448, 448)):  # Updated image size\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_processor = image_processor\n",
        "        self.max_length = max_length\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Load and process image\n",
        "        image = Image.open(f\"{img_folder_path}/{item['image']}\").convert('RGB')\n",
        "\n",
        "        # Process image with all required parameters\n",
        "        image_features = self.image_processor(\n",
        "            image,\n",
        "            return_tensors=\"pt\",\n",
        "            do_resize=True,\n",
        "            size={\"height\": self.image_size[0], \"width\": self.image_size[1]},\n",
        "            do_normalize=True\n",
        "        )\n",
        "\n",
        "        # Create prompt with system message\n",
        "        prompt = f\"<|im_start|>system\\nYou are a helpful assistant that answers math questions based on images.<|im_end|>\\n<|im_start|>user\\nQuestion: {item['question']}<|im_end|>\\n<|im_start|>assistant\\nLet me solve this step by step.\\nAnswer:\"\n",
        "        target = f\"{item['answer']}<|im_end|>\"\n",
        "\n",
        "        # Tokenize input and target\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        targets = self.tokenizer(\n",
        "            target,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'labels': targets['input_ids'].squeeze(),\n",
        "            'pixel_values': image_features['pixel_values'].squeeze(),\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "    labels = torch.stack([item['labels'] for item in batch])\n",
        "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': labels,\n",
        "        'pixel_values': pixel_values\n",
        "    }\n",
        "\n",
        "def train_model(train_data: List[Dict], validation_data: List[Dict], model_save_name: str):\n",
        "    # Initialize model, tokenizer, and image processor with trust_remote_code=True\n",
        "    model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=\"auto\", device_map=\"auto\"  # Enable automatic device mapping\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    image_processor = AutoImageProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "\n",
        "    # # Preparation for inference\n",
        "    # text = processor.apply_chat_template(\n",
        "    #     messages, tokenize=False, add_generation_prompt=True\n",
        "    # )\n",
        "    # image_inputs, video_inputs = process_vision_info(messages)\n",
        "    # inputs = processor(\n",
        "    #     text=[text],\n",
        "    #     images=image_inputs,\n",
        "    #     videos=video_inputs,\n",
        "    #     padding=True,\n",
        "    #     return_tensors=\"pt\",\n",
        "    # )\n",
        "    # inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    # # Inference: Generation of the output\n",
        "    # generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
        "    # generated_ids_trimmed = [\n",
        "    #     out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    # ]\n",
        "    # output_text = processor.batch_decode(\n",
        "    #     generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    # )\n",
        "\n",
        "    # Set output directory\n",
        "    output_dir = f\"./fine_tuned_{model_save_name}\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = MathVQADataset(train_data, tokenizer, image_processor)\n",
        "    val_dataset = MathVQADataset(validation_data, tokenizer, image_processor)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=2,  # Reduced batch size\n",
        "        per_device_eval_batch_size=2,   # Reduced batch size\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=os.path.join(output_dir, 'logs'),\n",
        "        logging_steps=100,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        save_steps=1000,\n",
        "        learning_rate=2e-5,\n",
        "        fp16=True,\n",
        "        gradient_accumulation_steps=8,  # Increased gradient accumulation\n",
        "        save_total_limit=3,\n",
        "        remove_unused_columns=False,\n",
        "        dataloader_num_workers=2\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=collate_fn,\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the final model\n",
        "    trainer.save_model()\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    image_processor.save_pretrained(output_dir)\n",
        "\n",
        "# Usage\n",
        "model_save_name = \"math_vqa_model\"\n",
        "train_model(train_data, val_data, model_save_name)"
      ],
      "metadata": {
        "id": "0W_KQmZieV7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(example, prompt_type ='full', context=''):\n",
        "    question = example['question']\n",
        "    options = ''\n",
        "    if len(example['options']) > 0:\n",
        "        assert len(example['options']) == 5, example\n",
        "        if ''.join(example['options']) != 'ABCDE':\n",
        "            options = f\"(A) {example['options'][0]}\\n(B) {example['options'][1]}\\n(C) {example['options'][2]}\\n(D) {example['options'][3]}\\n(E) {example['options'][4]}\\n\"\n",
        "\n",
        "    # input = 'Please solve the problem step by step and put your answer in one \"\\\\boxed{}\". If it is a multiple choice question, only one letter is allowed in the \"\\\\boxed{}\".\\n'+f\"{question}\\n{options}\"\n",
        "    if prompt_type == 'simple':\n",
        "      prompt = f\"{question}\\n{options}\\nAnswer the question using a single word or phrase.\"\n",
        "    else:\n",
        "      prompt = f\"\"\"{context}\n",
        "        Question: {question}\\n{options}\n",
        "        \"\"\" + 'Please solve the problem by elaborately, providing step-by-step reasoning for the solution and put your answer in one \"\\\\boxed{}. Also, provide the bounding box coordinate of the region that can help you answer the question better.If it is a multiple choice question, only one letter is allowed in the \\\\boxed{}.\\n'\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "GskEGYCeeb_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K8OmmbQMffA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "remove_list = ['the answer', 'is', ':', 'boxed', '$']\n",
        "\n",
        "results = pd.DataFrame(results)\n",
        "def final_answer(text: str):\n",
        "    text = text.lower()\n",
        "    if 'answer is' not in text:\n",
        "        return np.nan\n",
        "    # Create a regex pattern to match all words in the remove_list\n",
        "    pattern = r'(' + r'|'.join(re.escape(word) for word in remove_list) + r'|\\s)'\n",
        "    # Use re.sub to replace matched patterns with an empty string\n",
        "    result = re.sub(pattern, '', text.split('answer is')[-1])\n",
        "    result = re.sub(r'\\\\\\{(\\\\frac\\{[^{}]+\\}\\{[^{}]+\\})\\}', r'\\1', result)\n",
        "    return re.sub(r'\\\\\\\\', r'\\\\', result)\n",
        "# Example metric: String matching (very basic)\n",
        "results['prediction'] = results['generated_text'].apply(final_answer)\n",
        "results['exact_match'] = results['prediction'] == results['answer']\n",
        "display(results)\n",
        "pd.DataFrame(results).to_csv(f\"{img_folder_path}/test_generated_answer.csv\")\n",
        "\n",
        "accuracy = sum(results['exact_match']) / len(results)\n",
        "print(f\"Exact Match Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Optionally save results\n",
        "# import json\n",
        "# with open(\"evaluation_results.json\", \"w\") as f:\n",
        "#     json.dump(results, f, indent=4)"
      ],
      "metadata": {
        "id": "-rllWWvjk4Mm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}